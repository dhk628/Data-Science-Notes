\chapter{Transformers}

\begin{itemize}
\item 
Encoding component is a stack (six in original paper) of encoders with same structure but different weights. Decoding component is a stack of decoders of the same number.

\item 
Encoders: Self-Attention \textrightarrow{} FNN
	\begin{itemize}
	\item 
	First encoder receives embedding of words.
	
	\item 
	Other encoders receive output of encoder directly below.
	\end{itemize}

\item 
Decoders: Self-Attention \textrightarrow{} Encoder-Decoder Attention \textrightarrow{} FNN
\end{itemize}

\section{Ingredients}
\subsection{Self-attention}
\subsubsection{Process}
\begin{enumerate}
\item Create three vectors from each of the encoderâ€™s input vectors: Query, Key, and Value vectors.

\item Note: in original paper, input vectors are in $\R^{512},$ and QKV vectors are in $\R^{64}.$

\item Self-attention score between $i$-th and $j$-th word is $q_i \cdot k_j$ for all $j.$

\item Divide scores by square root of dimension (8; this gives more stable gradients) and softmax.

\item Multiply each value vector $v_j$ by softmax scores.

\item Sum up the weighted value vectors. This is output of self-attention layer for $i$-th word.
\end{enumerate}
In matrix form, suppose our input is $X \in \R^{n \times d_\model},$ where $n$ is the sequence length and $d_\model$ is the embedding dimension. (So rows of $X$ are the inputs.) The QKV matrices are given by $W_Q, W_K, W_V \in \R^{d_\model \times d_\k}.$ Then the QKV vectors
\begin{align*}
    Q &= XW_Q \\
    K &= XW_K \\
    V &= XW_V
\end{align*}
are in $\R^{n \times d_\k}$
For each attention head, we have
\[ \att(Q,K,V) = \softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) \times V \in \R^{n \times d_\k}, \]
i.e., aggregate values weighted by attention.

\subsection{Multi-headed attention}
Do $h$ parallel attention heads, with different weight matrices for each head.
\[ \operatorname{MultiHead}(Q, K, V) = \operatorname{Concat}(\head_1, \ldots, \head_h) \times W_O, \]
where
\[ \head_i = \att(Q_i, K_i, V_i) \]
is output of each attention head and $W_O \in \R^{hd_\k \times d_\model}.$ Note that $\operatorname{MultiHead}(Q, K, V) \in \R^{n \times d_\model}.$ 

\subsection{Positional encoding}
Transformers are permutation-invariant, so we add positional encoding to input embeddings.

\subsection{FNN}
Output of attention fed into FNN:
\[ \operatorname{FNN}(x) = \operatorname{ReLU}(xW_1 + b_1)W_2 + b_2. \]

\subsection{Residual connections \& layer normalization}
Each sub-layer (attention or feedforward) is followed by
\[ \layernorm(x + \operatorname{Sublayer}(x)). \]
Given $x \in \R^d,$ the layer norm is 
\[ \layernorm(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta, \]
where
\begin{gather*}
    \mu = \frac{\sum x_i}{d} \text{ (mean over features)}\\
    \sigma^2 = \frac{\sum(x_i - \mu)^2}{d} \text{ (variance over features)}\\
    \gamma,\beta \text{: learned scaling and shifting parameters}\\
    \epsilon \text{: small constant to prevent division by zero}.
\end{gather*}

\section{Original Architecture}
Encoder Layer:
\begin{itemize}
    \item Input Embedding + Positional Encoding
    \item Multi-Head Self-Attention \textrightarrow{} Residual + Layer Norm
    \item Feed-Forward Network \textrightarrow{} Residual + Layer Norm
\end{itemize}
Decoder Layer:
\begin{itemize}
\item Masked Multi-Head Self-Attention (prevent peeking at future tokens)
\item Multi-Head Encoder-Decoder Attention \textrightarrow{} Residual + Layer Norm
\item Feed-Forward Network \textrightarrow{} Residual + Layer Norm
\end{itemize}

\section{Modern Architecture}
\begin{itemize}
    \item Input Embedding
    \item Layer Norm \textrightarrow{} Self-Attention \textrightarrow{} Residual
        \begin{itemize}
            \item Grouped-Query Attention
            \item Rotary Embeddings
        \end{itemize}
    \item Layer Norm \textrightarrow{} FNN \textrightarrow{} Residual
\end{itemize}

\subsection{Grouped-query attention}
\begin{itemize}
    \item Use same $W_K$ and $W_V$ across heads, and each head has its own $W_Q.$
    \item Better: divide heads into groups. Heads in the same group share $W_K$ and $W_V$
\end{itemize}

\subsection{Rotary positional embeddings (RoPE)}
\begin{itemize}
	\item Limitations of absolute positional embedding:
	\begin{itemize}
		\item Limited sequence length
		\item Independence of positional embedding, e.g./ difference between position 1 and 2 is the same as the difference between position 1 and 500
	\end{itemize}
    \item Alternative: \textit{relative positional embeddings} \cite{relposrep}
    \begin{itemize}
        \item Bias for positional offsets: use a bias to represent each possible positional offset
        \item Relative attention becomes:
        \begin{equation*}
            \att(Q,K,V)_i = \sum_{j=1}^{n}\softmax(e_{ij}) \times (x_jW_V + a_{ij}^V),
        \end{equation*}
        where
        \begin{equation*}
            e_{ij} = \frac{QK^T + x_iW_Q(a_{ij}^K)^T}{\sqrt{d_k}}.
        \end{equation*}
        \item Here, $a_{ij} \in \R^{1\times d_a}$ is a vector of relative positional weights, i.e.,
        \begin{gather*}
            a_{ij} = w_{\operatorname{clip}(j-i,k)}\\
            \operatorname{clip}(x,k) = \max(-k, \min(k,x)).
        \end{gather*}
        Calculate $w$ for both keys and values.
        \item Clipping allows scalability (i.e., arbitrarily long sequences)
        \item Limitations: slower; complicates key-value cache usage as each additional token changes the embedding for every other token.
    \end{itemize}
    \item For \textbf{RoPE} \cite{rotpos}, the intuition is to rotate each embedding by $m \theta,$ where $m$ is the position of the word in the sequence.
    \item Benefits:
        \begin{itemize}
            \item Scalability: adding new words does not change the embedding of previous words
            \item The dot product of the embeddings of two words does not depend on absolute position.
        \end{itemize}
    \item Mathematically, we first work in $\C^{d_k/2}.$ Let $M_j$ be the rotation matrix by $m \theta_j.$ Then the output of RoPE for the $m$-th word is just
    \begin{equation*}
        Q_m\Theta_m = Q_m \times
        \begin{pmatrix}
        M_1 & & & \\
        & M_2 & & \\
        & & \ddots & \\
        & & & M_{d_k/2}
        \end{pmatrix},
    \end{equation*}
    where $Q_m$ is the $m$-th row of $Q$ (i.e., query vector for $m$-th word).
    \item Do the same for key vector.
\end{itemize}

\section{Other Techniques}
\subsection{Sparse attention}
Instead of global autoregressive self-attention, use local autoregressive self-attention in some layers.

\subsection{Mixture of experts (MOE)}
\begin{itemize}
    \item Instead of a single monolithic feedforward layer in the transformer block, use a set of expert networks, and route the input to only a few of them.
    \item A router (a smaller FNN) calculates which experts to turn on.
    \item Could do model merging by doing a weighted average of experts using weights from the router.
\end{itemize}

\section{Complexity}

\subsection{Number of parameters and FLOPs}

Suppose input length is $n$ and $n_\layer$ is the number of layers. Recall $X \in \R^{n \times d_\model};$ $W_Q,W_K,W_V \in \R^{d_\model \times d_\k};$ and $Q,K,V \in \R^{n\times d_\k}.$ As in \cite{scaling_laws}, we have the following steps in the transformer architecture:
\begin{enumerate}
    \item Embed: token and positional embeddings.
        \begin{itemize}
            \item Notation:
            \begin{itemize}
                \item $d_\model$ is model dimension, i.e., dimension of the residual stream.
                \item $n_\vocab$ is the number of tokens in the vocabulary. 
            \end{itemize}
            \item Parameters: $n_\vocab \times d_\model$ for token embedding, and $n \times d_\model$ for positional embedding $\Rightarrow (n_\vocab + n)d_\model$ parameters.
            \item FLOPs: $O(d_\model)$ per token
        \end{itemize}
    \item Attention (QKV): project into QKV vectors.
        \begin{itemize}
            \item Notation:
            \begin{itemize}
                \item $d_\k = d_\v$ is the dimension of the projections.
                \item $n_\heads$ is the number of attention heads.
                \item $d_\attn = d_kn_\heads$ is the output of the multi-headed attention (usually $=d_\model$).
            \end{itemize}
            \item Parameters: $3d_\model d_\k$ per head per layer $\Rightarrow 3n_\layer d_\model d_\attn$ parameters.
            \item FLOPs: we are doing three instances of $[n, d_\model] \times [d_\model , d_\k],$ so get $3\times 2 \times nd_\model d_\k$ per head per layer $\Rightarrow 6d_\model d_\attn$ per token per layer $\Rightarrow 6n_\layer d_\model d_\attn$ per token.
        \end{itemize}
    \item Attention (Mask): dot-product query and key, i.e., $QK^T.$
        \begin{itemize}
            \item FLOPs: doing $[n, d_\k] \times [d_\k, n],$ so get $2n^2d_\k$ per head per layer $\Rightarrow 2nn_\layer d_\attn$ per token.
        \end{itemize}
    \item Attention (Project): project output of attention heads to $d_\model$ by multiplying with $W_O \in \R^{d_\attn \times d_\model}$
        \begin{itemize}
            \item Parameters: $d_\attn d_\model$ per layer $\Rightarrow n_\layer d_\attn d_\model$ parameters.
            \item FLOPs: doing $[n,d_\attn] \times [d_\attn, d_\model],$ so get $2nd_\attn d_\model$ per layer $\Rightarrow 2n_\layer d_\attn d_\model$ per token.
        \end{itemize}
    \item Feedforward: two linear layers in dense neural network.
        \begin{itemize}
            \item Notation:
            \begin{itemize}
                \item $d_\ff$ is output size of first linear layer (usually $d_\ff = 4d_\model$).
            \end{itemize}
            \item Parameters: first weight matrix gives $d_\model d_\ff$ and second gives $d_\ff d_\model$ per layer $\Rightarrow 2n_\layer d_\model d_\ff$ parameters.
            \item FLOPs: doing $[n, d_\model] \times [d_\model, d_\ff] \times [d_\ff, d_\model]$ per layer, so get $4nd_\model d_\ff$ per layer $\Rightarrow 4n_\layer d_\model d_\ff$ per token.
        \end{itemize}
    \item De-embed: go back to vocabulary.
        \begin{itemize}
            \item FLOPs: doing $[n, d_\model] \times [d_\model, d_\vocab],$ so get $2nd_\model d_\vocab \Rightarrow 2d_\model d_\vocab$ per token.
        \end{itemize}
\end{enumerate}
Note that we are doing FLOPs \textit{per token}. So the total number of parameters is
\begin{equation*}
    2n_\layer d_\model (2d_\attn + d_\ff) + d_\model(n_\vocab + n) \approx 2n_\layer d_\model (2d_\attn + d_\ff) \eqqcolon N
\end{equation*}
and the total FLOPs per token for the forward pass becomes
\begin{equation*}
    2N + 2d_\model d_\vocab + 2n n_\layer d_\attn \approx 2N + 2n n_\layer d_\attn \eqqcolon C_\forward.
\end{equation*}
The FLOPs for the backwards pass is about twice the FLOPs for the forward pass, so the total number of FLOPs per token is
\begin{equation*}
    C \approx 6N.
\end{equation*}
Then the total number of FLOPs is
\begin{equation*}
    6ND,
\end{equation*}
where $D$ is the number of tokens in the corpus.

If we let $\tau$ be the throughput, i.e., inputs processed per unit time (in FLOPs per second) and $T$ be the total training time, then we get
\begin{equation*}
    \tau T \approx 6ND
\end{equation*}

\subsection{Time complexity of self-attention}

\begin{itemize}
    \item Calculating $Q,K,V$ are all $O(nd_\model d_\k).$
    \item Calculating $QK^T$ is $O(n^2d_\k).$
    \item Doing softmax is $O(n^2).$
    \item Multiplying result with $V$ is $O(n^2d_\k).$
\end{itemize}
Overall, the time complexity of the self-attention mechanism is $O(nd_\model d_\k + n^2d_\k).$ If $n \gg d_\model,d_\k,$ this reduces to just $O(n^2).$

\begin{remark*}
    In \cite{keles2022computationalcomplexityselfattention}, the authors show that the time complexity of self-attention is necessarily quadratic in the input length, unless the Strong Exponential Time Hypothesis (SETH) is false.
\end{remark*}
